{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfa68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ClimbsFeatureArray...\n",
      "ClimbsFeatureArray initialized! 132057 unique climbs added!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.1117, -0.5647],\n",
       "         [-0.0870, -0.5405, -0.0031, -0.0611],\n",
       "         [ 0.0000, -0.5405, -0.0031, -0.0611],\n",
       "         [-0.1739, -0.2162,  0.0357,  0.0094],\n",
       "         [-0.1304,  0.1622, -0.3267, -0.5005],\n",
       "         [-0.0435,  0.2703, -0.0112, -0.4194],\n",
       "         [ 0.1304,  0.4865,  0.2634, -0.5250],\n",
       "         [ 0.0000,  0.6486, -0.2856, -0.8135],\n",
       "         [ 0.3913,  0.2703, -0.5112, -0.3729],\n",
       "         [ 0.0435,  0.8108,  0.2071, -0.8304],\n",
       "         [ 0.0000,  0.8649,  0.0413, -0.8501],\n",
       "         [ 0.1304,  1.1351,  0.5235, -0.7133],\n",
       "         [-0.1739,  1.2973,  0.0599, -0.8491],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000],\n",
       "         [-2.0000,  0.0000, -2.0000,  0.0000]]),\n",
       " tensor([-0.4788, -0.9626,  0.4949,  0.1429]),\n",
       " tensor([[1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from climb_conversion import ClimbsFeatureArray, ClimbsFeatureScaler\n",
    "from hold_classifier import UNetHoldClassifierLogits\n",
    "from simple_diffusion import ClimbDDPM, Noiser, zero_com\n",
    "\n",
    "climbs = ClimbsFeatureArray()\n",
    "dataset = climbs.get_features(limit = 500, roles=True)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d664dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [02:42<00:29,  1.95s/it, Epoch: 84: Avg Batch Loss: 0.000, Min Batch Loss: 0.000. 7 batches, 2307589 params. New best mean batch loss! Saving hold classifier at data/weights/unet-hold-classifier...]"
     ]
    }
   ],
   "source": [
    "class ResidualBlock1D_V2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cond_dim, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=padding)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.cond_proj = nn.Linear(cond_dim, out_channels*2)\n",
    "        self.shortcut = nn.Conv1d(in_channels,out_channels,1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "\n",
    "        gamma, beta = self.cond_proj(cond).unsqueeze(-1).chunk(2, dim=1)\n",
    "        h = h*(1+gamma) + beta\n",
    "        h = self.act(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "class UNetHoldClassifierLogits(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features_dim: int = 4,\n",
    "        in_cond_dim: int = 4,\n",
    "        out_dim: int = 5,\n",
    "        hidden_dim: int = 128,\n",
    "        n_layers: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cond_emb = nn.Sequential(\n",
    "            nn.Linear(in_cond_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.init_conv = ResidualBlock1D_V2(in_features_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([ResidualBlock1D_V2(hidden_dim*(i+1), hidden_dim*(i+2), hidden_dim) for i in range(n_layers)])\n",
    "        self.up_blocks = nn.ModuleList([ResidualBlock1D_V2(hidden_dim*(i+1), hidden_dim*(i), hidden_dim) for i in range(n_layers,0,-1)])\n",
    "\n",
    "        self.head = nn.Conv1d(hidden_dim, out_dim, 1)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "\n",
    "        x = zero_com(x, 2)\n",
    "\n",
    "        cond_emb = self.cond_emb(cond)\n",
    "        h_emb = self.init_conv(x.transpose(1,2), cond_emb)\n",
    "\n",
    "        residuals = []\n",
    "        for layer in self.down_blocks:\n",
    "            residuals.append(h_emb)\n",
    "            h_emb = layer(h_emb, cond_emb)\n",
    "        \n",
    "        for layer in self.up_blocks:\n",
    "            resid = residuals.pop()\n",
    "            h_emb = resid + layer(h_emb, cond_emb)\n",
    "        \n",
    "        h_out = self.head(h_emb).transpose(1,2)\n",
    "\n",
    "        return h_out\n",
    "\n",
    "def train_unet_hold_classifier_logits(\n",
    "    hold_classifier: nn.Module,\n",
    "    dataset: TensorDataset,\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 0,\n",
    "    save_path: str | None = None,\n",
    "    save_on_best: bool = True,\n",
    "    torch_compile: bool = False\n",
    ") -> tuple[nn.Module, list[float]]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Model Set-Up\n",
    "    hold_classifier.to(device)\n",
    "    hold_classifier.train()\n",
    "    if torch_compile:\n",
    "        hold_classifier = torch.compile(hold_classifier)\n",
    "    optimizer = torch.optim.Adam(params = hold_classifier.parameters())\n",
    "\n",
    "    n_params = sum([p.numel() for p in hold_classifier.parameters()])\n",
    "\n",
    "    # DataLoader Set-Up\n",
    "    batches = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    epoch_losses = []\n",
    "    with tqdm(range(epochs)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            batch_losses = []\n",
    "            for x, cond, target_role_probs in batches:\n",
    "                x, cond = x.to(device), cond.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred_logits = hold_classifier(x, cond)\n",
    "\n",
    "                loss = F.cross_entropy(pred_logits.transpose(1,2), torch.argmax(target_role_probs,dim=2))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "            mean_batch_loss = sum(batch_losses)/len(batch_losses)\n",
    "            epoch_losses.append(mean_batch_loss)\n",
    "            info_str = f\"Epoch: {epoch}: Avg Batch Loss: {mean_batch_loss:.3f}, Min Batch Loss: {min(epoch_losses):.3f}. {len(batch_losses)} batches, {n_params} params\"\n",
    "            if save_path and save_on_best and (min(epoch_losses)==mean_batch_loss):\n",
    "                pbar.set_postfix_str(f\"{info_str}. New best mean batch loss! Saving hold classifier at {save_path}...\")\n",
    "                torch.save(hold_classifier.state_dict(), save_path)\n",
    "            else:\n",
    "                pbar.set_postfix_str(info_str)\n",
    "\n",
    "    if save_path:\n",
    "        print(f\"Saving hold classifier at {save_path}...\")\n",
    "        torch.save(hold_classifier.state_dict(), save_path)\n",
    "    \n",
    "    \n",
    "    # Plot training results\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(list(range(len(epoch_losses))), epoch_losses)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Mean Batch-Loss per Epoch, U-Net Hold Classifier ({n_params} params)')\n",
    "    plt.show()\n",
    "\n",
    "    return hold_classifier, epoch_losses\n",
    "\n",
    "CLASSIFIER_SAVE_PATH = \"data/weights/unet-hold-classifier\"\n",
    "\n",
    "train_unet_hold_classifier_logits(\n",
    "    UNetHoldClassifierLogits(),\n",
    "    dataset,\n",
    "    save_path=CLASSIFIER_SAVE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in UNetHoldClassifierLogits(n_layers=3).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "WALL_ID = 'wall-443c15cd12e0'\n",
    "\n",
    "\n",
    "class EGNNHoldClassifier(nn.Module):\n",
    "    def __init__(self, weights_path: str | None = None, input_dim: int = 8, hidden_dim: int = 256, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout = 0.3,\n",
    "            bidirectional=True,\n",
    "            device=self.device,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if weights_path:\n",
    "            self.load_state_dict(torch.load(weights_path, map_location = self.device))\n",
    "    \n",
    "    def loss(self, pred_roles: Tensor, true_roles: Tensor):\n",
    "        \"\"\"Get the loss from the model's predictions, via cross-entropy loss.\"\"\"\n",
    "        return self.loss_func(pred_roles, true_roles)\n",
    "\n",
    "    \n",
    "    def forward(self, holds_cond: PackedSequence | Tensor)-> Tensor:\n",
    "        \"\"\"Run the forward pass. Predicts the roles for a given (possibly batched) set of holds, given (possibly batched) wall conditions.\"\"\"\n",
    "\n",
    "        _, (hs, cs) = self.lstm(holds_cond)\n",
    "\n",
    "        lstm_final_state = torch.cat([hs[-1],hs[-2]], dim=1)\n",
    "        \n",
    "        sf_logits = self.classification_head(lstm_final_state)\n",
    "\n",
    "        return sf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6f6b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EvanM\\Documents\\Projects\\GitHub\\ml-homewall-climb-generator\\model-training\\simple_diffusion.py:520: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filepath, map_location=map_loc)\n",
      "c:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "C:\\Users\\EvanM\\AppData\\Local\\Temp\\ipykernel_13008\\1567719866.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(weights_path, map_location = self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[640, 0],\n",
       " [897, 2],\n",
       " [741, 2],\n",
       " [935, 2],\n",
       " [874, 2],\n",
       " [939, 2],\n",
       " [748, 2],\n",
       " [817, 2],\n",
       " [788, 1]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "GRADE_TO_DIFF = {\n",
    "    \"font\": {\n",
    "        \"4a\": 10, \"4b\": 11, \"4c\": 12,\n",
    "        \"5a\": 13, \"5b\": 14, \"5c\": 15,\n",
    "        \"6a\": 16, \"6a+\": 17, \"6b\": 18, \"6b+\": 19,\n",
    "        \"6c\": 20, \"6c+\": 21,\n",
    "        \"7a\": 22, \"7a+\": 23, \"7b\": 24, \"7b+\": 25,\n",
    "        \"7c\": 26, \"7c+\": 27,\n",
    "        \"8a\": 28, \"8a+\": 29, \"8b\": 30, \"8b+\": 31,\n",
    "        \"8c\": 32, \"8c+\": 33,\n",
    "    },\n",
    "    \"v_grade\": {\n",
    "        \"V0-\": 10, \"V0\": 11, \"V0+\": 12,\n",
    "        \"V1\": 13, \"V1+\": 14, \"V2\": 15,\n",
    "        \"V3\": 16, \"V3+\": 17, \"V4\": 18, \"V4+\": 19,\n",
    "        \"V5\": 20, \"V5+\": 21, \"V6\": 22, \"V6+\": 22.5,\n",
    "        \"V7\": 23, \"V7+\": 23.5, \"V8\": 24, \"V8+\": 25,\n",
    "        \"V9\": 26, \"V9+\": 26.5, \"V10\": 27, \"V10+\": 27.5,\n",
    "        \"V11\": 28, \"V11+\": 28.5, \"V12\": 29, \"V12+\": 29.5,\n",
    "        \"V13\": 30, \"V13+\": 30.5, \"V14\": 31, \"V14+\": 31.5,\n",
    "        \"V15\": 32, \"V15+\": 32.5, \"V16\": 33,\n",
    "    },\n",
    "}\n",
    "\n",
    "class ClimbDDPMGenerator():\n",
    "    def __init__(\n",
    "            self,\n",
    "            wall_id: str,\n",
    "            scaler: ClimbsFeatureScaler,\n",
    "            ddpm: ClimbDDPM,\n",
    "            role_classifer: HoldClassifier | None = None\n",
    "        ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.scaler = scaler\n",
    "        self.ddpm = ddpm\n",
    "        self.timesteps = 100\n",
    "\n",
    "        if role_classifer:\n",
    "            self.role_classifier = role_classifer\n",
    "\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            holds = pd.read_sql_query(\"SELECT hold_index, x, y, pull_x, pull_y, useability, is_foot, wall_id FROM holds WHERE wall_id = ?\",conn,params=(wall_id,))\n",
    "            scaled_holds = self.scaler.transform_hold_features(holds, to_df=True)\n",
    "            self.holds_manifold = torch.tensor(scaled_holds[['x','y','pull_x','pull_y']].values, dtype=torch.float32)\n",
    "            self.holds_lookup = scaled_holds['hold_index'].values\n",
    "        \n",
    "        self.holds_lookup = np.concatenate([self.holds_lookup, np.array([-1, -1, -1, -1])])\n",
    "        \n",
    "        self.holds_manifold = torch.cat([\n",
    "            self.holds_manifold,\n",
    "            torch.tensor(\n",
    "                [[-2.0, 0.0, -2.0, 0.0],\n",
    "                [2.0, 0.0, -2.0, 0.0],\n",
    "                [-2.0, 0.0, 2.0, 0.0],\n",
    "                [2.0, 0.0, 2.0, 0.0]],dtype=torch.float32)\n",
    "            ],dim=0)\n",
    "\n",
    "    def _build_cond_tensor(self, n, grade, diff_scale, angle):\n",
    "        diff = GRADE_TO_DIFF[diff_scale][grade]\n",
    "        df_cond = pd.DataFrame({\n",
    "            \"grade\": [diff]*n,\n",
    "            \"quality\": [2.9]*n,\n",
    "            \"ascents\": [100]*n,\n",
    "            \"angle\": [angle]*n\n",
    "        })\n",
    "\n",
    "        cond = self.scaler.transform_climb_features(df_cond).T\n",
    "        return torch.tensor(cond, device=self.device, dtype=torch.float32)\n",
    "    \n",
    "    def _project_onto_manifold(self, gen_climbs: Tensor, offset_manifold: Tensor)-> Tensor:\n",
    "        \"\"\"\n",
    "            Project each generated hold to its nearest neighbor on the hold manifold.\n",
    "            \n",
    "            Args:\n",
    "                gen_climbs: (B, S, H) predicted clean holds\n",
    "                return_indices: (boolean) Whether to return the hold indices or hold feature coordinates\n",
    "            Returns:\n",
    "                projected: (B, S, H) each hold snapped to nearest manifold point\n",
    "        \"\"\"\n",
    "        B, S, H = gen_climbs.shape\n",
    "        flat_climbs = gen_climbs.reshape(-1,H)\n",
    "        dists = torch.cdist(flat_climbs, offset_manifold)\n",
    "        idx = dists.argmin(dim=1)\n",
    "        return self.holds_manifold[idx].reshape(B, S, -1)\n",
    "        \n",
    "    def _project_onto_indices(self, gen_climbs: Tensor, offset_manifold: Tensor):\n",
    "        \"\"\"Project climb onto the final hold indices (and remove null holds)\"\"\"\n",
    "        \n",
    "        B, S, H = gen_climbs.shape\n",
    "\n",
    "        climbs = []\n",
    "        for gen_climb in gen_climbs:\n",
    "            flat_climb = gen_climb.reshape(-1,H)\n",
    "            dists = torch.cdist(flat_climb, offset_manifold)\n",
    "            idx = dists.argmin(dim=1)\n",
    "            idx = idx.detach().numpy()\n",
    "            holds = self.holds_lookup[idx]\n",
    "            climb = list(set(holds[holds > 0].tolist()))\n",
    "            climbs.append(climb)  \n",
    "        return climbs\n",
    "    \n",
    "    def _projection_strength(self, t: Tensor, t_start_projection: float = 0.3):\n",
    "        \"\"\"Calculate the weight to assign to the projected holds based on the timestep.\"\"\"\n",
    "        a = (t_start_projection-t)/t_start_projection\n",
    "        strength = 1 - torch.cos(a*torch.pi/2)\n",
    "        return torch.where(t > t_start_projection, torch.zeros_like(t), strength)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        n: int = 1 ,\n",
    "        angle: int = 45,\n",
    "        grade: str = 'V4',\n",
    "        diff_scale: str = 'v_grade',\n",
    "        deterministic: bool = False,\n",
    "        classify_holds: bool = False\n",
    "    )->list[list[int]]:\n",
    "        \"\"\"\n",
    "        Generate a climb or batch of climbs with the given conditions using the standard DDPM iterative denoising process.\n",
    "        \n",
    "        :param n: Number of climbs to generate\n",
    "        :type n: int\n",
    "        :param angle: Angle of the wall\n",
    "        :type angle: int\n",
    "        :param grade: Desired difficulty (V-grade)\n",
    "        :type grade: int | None\n",
    "        :return: A Tensor containing the denoised generated climbs as hold sets.\n",
    "        :rtype: Tensor\n",
    "        \"\"\"\n",
    "        cond_t = self._build_cond_tensor(n, grade, diff_scale, angle)\n",
    "        x_t = torch.randn((n, 20, 4), device=self.device)\n",
    "        noisy = x_t.clone()\n",
    "        t_tensor = torch.ones((n,1), device=self.device)\n",
    "        \n",
    "        # Randomly offset the holds-manifold to allow for climbs to be generated at different x-coordinates around the wall.\n",
    "        x_offset = np.random.randn()*0.2\n",
    "        offset_manifold = self.holds_manifold.clone()\n",
    "        offset_manifold[:,0] += x_offset\n",
    "\n",
    "        for t in range(0, self.timesteps):\n",
    "            print('.',end='')\n",
    "\n",
    "            gen_climbs = self.ddpm(noisy, cond_t, t_tensor)\n",
    "\n",
    "            alpha_p = self._projection_strength(t_tensor)\n",
    "            projected_climbs = self._project_onto_manifold(gen_climbs, offset_manifold)\n",
    "            gen_climbs = alpha_p*(projected_climbs) + (1-alpha_p)*(gen_climbs)\n",
    "            \n",
    "            t_tensor -= 1.0/self.timesteps\n",
    "            noisy = self.ddpm.forward_diffusion(gen_climbs, t_tensor, x_t if deterministic else torch.randn_like(x_t))\n",
    "        \n",
    "        hold_indices_out = self._project_onto_indices(gen_climbs, offset_manifold)\n",
    "        \n",
    "        if classify_holds:\n",
    "            S = len(hold_indices_out[0])\n",
    "            # Convert gen_climbs into an input dataset for the HoldClassifier\n",
    "            gen_climbs[:,:,0] -= x_offset\n",
    "            input_seq = gen_climbs.reshape(-1,4)\n",
    "            input_seq = input_seq[:S,:]\n",
    "            input_seq = torch.cat([input_seq, cond_t.expand(S,-1)], dim=1).unsqueeze(0)\n",
    "            \n",
    "            # Run the classifier\n",
    "            sf_logits = self.role_classifier(input_seq)\n",
    "\n",
    "            # Assign hold roles, assigning start and finish roles based on HoldClassifier's best guess.\n",
    "            dual_start = sf_logits[0][0] > 0\n",
    "            dual_fin = sf_logits[0][1] > 0\n",
    "\n",
    "            roles = [[idx,2] for idx in hold_indices_out[0]]\n",
    "            roles[0][1] = 0\n",
    "            roles[-1][1] = 1\n",
    "            if dual_start:\n",
    "                roles[1][1] = 0\n",
    "            if dual_fin:\n",
    "                roles[-2][1] = 1\n",
    "\n",
    "            return roles\n",
    "        else:\n",
    "            return hold_indices_out\n",
    "\n",
    "model = ClimbDDPM(\n",
    "    model=Noiser(),\n",
    "    weights_path=DDPM_WEIGHTS_PATH,\n",
    "    timesteps=100,\n",
    ")\n",
    "scaler = ClimbsFeatureScaler(\n",
    "    weights_path=SCALER_WEIGHTS_PATH\n",
    ")\n",
    "role_classifier = HoldClassifier(LSTM_WEIGHTS_PATH)\n",
    "generator = ClimbDDPMGenerator(\n",
    "    wall_id=WALL_ID,\n",
    "    scaler=scaler,\n",
    "    ddpm=model,\n",
    "    role_classifer=role_classifier\n",
    ")\n",
    "generator.generate(classify_holds=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
