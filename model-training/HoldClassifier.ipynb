{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfa68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from climb_conversion import ClimbsFeatureArray, ClimbsFeatureScaler\n",
    "from hold_classifier import UNetHoldClassifierLogits, train_unet_hold_classifier_logits\n",
    "from simple_diffusion import ClimbDDPM, Noiser, zero_com\n",
    "\n",
    "# CLASSIFIER_SAVE_PATH = \"data/weights/unet-hold-classifier.pth\"\n",
    "\n",
    "# climbs = ClimbsFeatureArray()\n",
    "# dataset = climbs.get_features(roles=True)\n",
    "\n",
    "# hold_classifier = UNetHoldClassifierLogits()\n",
    "# train_unet_hold_classifier_logits(\n",
    "#     hold_classifier,\n",
    "#     dataset,\n",
    "#     epochs=20,\n",
    "#     batch_size=1024,\n",
    "#     num_workers=2,\n",
    "#     save_path= CLASSIFIER_SAVE_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d664dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EvanM\\Documents\\Projects\\GitHub\\ml-homewall-climb-generator\\model-training\\simple_diffusion.py:612: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filepath, map_location=map_loc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x2 and 4x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 187\u001b[39m\n\u001b[32m    178\u001b[39m hold_classifier = UNetHoldClassifierLogits(\n\u001b[32m    179\u001b[39m     weights_path=HC_WEIGHTS_PATH\n\u001b[32m    180\u001b[39m )\n\u001b[32m    181\u001b[39m generator = ClimbDDPMGenerator(\n\u001b[32m    182\u001b[39m     wall_id=WALL_ID,\n\u001b[32m    183\u001b[39m     scaler=scaler,\n\u001b[32m    184\u001b[39m     ddpm=ddpm,\n\u001b[32m    185\u001b[39m     hold_classifier=hold_classifier\n\u001b[32m    186\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m climbs = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m climbs:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28mprint\u001b[39m(c)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mClimbDDPMGenerator.generate\u001b[39m\u001b[34m(self, n, angle, grade, diff_scale, deterministic)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.timesteps):\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m,end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     gen_climbs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     alpha_p = \u001b[38;5;28mself\u001b[39m._projection_strength(t_tensor)\n\u001b[32m    161\u001b[39m     projected_climbs = \u001b[38;5;28mself\u001b[39m._project_onto_manifold(gen_climbs, offset_manifold)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\Documents\\Projects\\GitHub\\ml-homewall-climb-generator\\model-training\\simple_diffusion.py:313\u001b[39m, in \u001b[36mClimbDDPM.forward\u001b[39m\u001b[34m(self, noisy, cond, t)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noisy, cond, t):\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\Documents\\Projects\\GitHub\\ml-homewall-climb-generator\\model-training\\simple_diffusion.py:303\u001b[39m, in \u001b[36mClimbDDPM.predict_clean\u001b[39m\u001b[34m(self, noisy, cond, t)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return predicted clean data.\"\"\"\u001b[39;00m\n\u001b[32m    302\u001b[39m a = \u001b[38;5;28mself\u001b[39m._cos_alpha_bar(t)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m prediction = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m clean = (noisy - torch.sqrt(\u001b[32m1\u001b[39m-a)*prediction)/torch.sqrt(a)\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clean\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\Documents\\Projects\\GitHub\\ml-homewall-climb-generator\\model-training\\simple_diffusion.py:202\u001b[39m, in \u001b[36mNoiser.forward\u001b[39m\u001b[34m(self, climbs, cond, t)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[33;03mRun denoising pass. Predicts the added noise from the noisy data.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m \u001b[33;03m:param t: Tensor with timestep of diffusion. [B, 1]\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    201\u001b[39m emb_t = \u001b[38;5;28mself\u001b[39m.time_mlp(t)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m emb_c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcond_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m emb_c = \u001b[38;5;28mself\u001b[39m.combine_t_mlp(torch.cat([emb_t, emb_c],dim=\u001b[32m1\u001b[39m))\n\u001b[32m    204\u001b[39m emb_h = \u001b[38;5;28mself\u001b[39m.init_conv(climbs.transpose(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m), emb_c)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (4x2 and 4x128)"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"data/storage.db\"\n",
    "SCALER_WEIGHTS_PATH = 'data/weights/climbs-feature-scaler.joblib'\n",
    "DDPM_WEIGHTS_PATH = 'data/weights/simple-diffusion-large.pth'\n",
    "HC_WEIGHTS_PATH = 'data/weights/unet-hold-classifier.pth'\n",
    "WALL_ID = 'wall-0a877f13d8e5'\n",
    "\n",
    "GRADE_TO_DIFF = {\n",
    "    \"font\": {\n",
    "        \"4a\": 10, \"4b\": 11, \"4c\": 12,\n",
    "        \"5a\": 13, \"5b\": 14, \"5c\": 15,\n",
    "        \"6a\": 16, \"6a+\": 17, \"6b\": 18, \"6b+\": 19,\n",
    "        \"6c\": 20, \"6c+\": 21,\n",
    "        \"7a\": 22, \"7a+\": 23, \"7b\": 24, \"7b+\": 25,\n",
    "        \"7c\": 26, \"7c+\": 27,\n",
    "        \"8a\": 28, \"8a+\": 29, \"8b\": 30, \"8b+\": 31,\n",
    "        \"8c\": 32, \"8c+\": 33,\n",
    "    },\n",
    "    \"v_grade\": {\n",
    "        \"V0-\": 10, \"V0\": 11, \"V0+\": 12,\n",
    "        \"V1\": 13, \"V1+\": 14, \"V2\": 15,\n",
    "        \"V3\": 16, \"V3+\": 17, \"V4\": 18, \"V4+\": 19,\n",
    "        \"V5\": 20, \"V5+\": 21, \"V6\": 22, \"V6+\": 22.5,\n",
    "        \"V7\": 23, \"V7+\": 23.5, \"V8\": 24, \"V8+\": 25,\n",
    "        \"V9\": 26, \"V9+\": 26.5, \"V10\": 27, \"V10+\": 27.5,\n",
    "        \"V11\": 28, \"V11+\": 28.5, \"V12\": 29, \"V12+\": 29.5,\n",
    "        \"V13\": 30, \"V13+\": 30.5, \"V14\": 31, \"V14+\": 31.5,\n",
    "        \"V15\": 32, \"V15+\": 32.5, \"V16\": 33,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class ClimbDDPMGenerator():\n",
    "    def __init__(\n",
    "            self,\n",
    "            wall_id: str,\n",
    "            scaler: ClimbsFeatureScaler,\n",
    "            ddpm: ClimbDDPM,\n",
    "            hold_classifier: UNetHoldClassifierLogits,\n",
    "        ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.scaler = scaler\n",
    "        self.ddpm = ddpm\n",
    "        self.hold_classifier = hold_classifier\n",
    "        self.timesteps = 100\n",
    "\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            holds = pd.read_sql_query(\"SELECT hold_index, x, y, pull_x, pull_y, useability, is_foot, wall_id FROM holds WHERE wall_id = ?\",conn,params=(wall_id,))\n",
    "            scaled_holds = self.scaler.transform_hold_features(holds, to_df=True)\n",
    "            self.holds_manifold = torch.tensor(scaled_holds[['x','y','pull_x','pull_y']].values, dtype=torch.float32)\n",
    "            self.holds_lookup = scaled_holds['hold_index'].values\n",
    "        \n",
    "        self.holds_lookup = np.concatenate([self.holds_lookup, np.array([-1, -1, -1, -1])])\n",
    "        \n",
    "        self.holds_manifold = torch.cat([\n",
    "            self.holds_manifold,\n",
    "            torch.tensor(\n",
    "                [[-2.0, 0.0, -2.0, 0.0],\n",
    "                [2.0, 0.0, -2.0, 0.0],\n",
    "                [-2.0, 0.0, 2.0, 0.0],\n",
    "                [2.0, 0.0, 2.0, 0.0]],dtype=torch.float32)\n",
    "            ],dim=0)\n",
    "\n",
    "    def _build_cond_tensor(self, n, grade, diff_scale, angle):\n",
    "        diff = GRADE_TO_DIFF[diff_scale][grade]\n",
    "        df_cond = pd.DataFrame({\n",
    "            \"grade\": [diff]*n,\n",
    "            \"quality\": [2.9]*n,\n",
    "            \"ascents\": [100]*n,\n",
    "            \"angle\": [angle]*n\n",
    "        })\n",
    "\n",
    "        cond = self.scaler.transform_climb_features(df_cond).T\n",
    "        return torch.tensor(cond, device=self.device, dtype=torch.float32)\n",
    "    \n",
    "    def _project_onto_manifold(self, gen_climbs: Tensor, offset_manifold: Tensor)-> Tensor:\n",
    "        \"\"\"\n",
    "            Project each generated hold to its nearest neighbor on the hold manifold.\n",
    "            \n",
    "            Args:\n",
    "                gen_climbs: (B, S, H) predicted clean holds\n",
    "                return_indices: (boolean) Whether to return the hold indices or hold feature coordinates\n",
    "            Returns:\n",
    "                projected: (B, S, H) each hold snapped to nearest manifold point\n",
    "        \"\"\"\n",
    "        B, S, H = gen_climbs.shape\n",
    "        flat_climbs = gen_climbs.reshape(-1,H)\n",
    "        dists = torch.cdist(flat_climbs, offset_manifold)\n",
    "        idx = dists.argmin(dim=1)\n",
    "        return self.holds_manifold[idx].reshape(B, S, -1)\n",
    "        \n",
    "    def _project_onto_indices(self, gen_climbs: Tensor, cond_t: Tensor, offset_manifold: Tensor) -> list[list[int]]:\n",
    "        \"\"\"Project climb onto the final hold indices (and remove null holds)\"\"\"\n",
    "        \n",
    "        B, S, H = gen_climbs.shape\n",
    "\n",
    "        roles = torch.argmax(self.hold_classifier(gen_climbs, cond_t), dim=2).detach().numpy()\n",
    "\n",
    "        flat_climbs = gen_climbs.reshape(-1,H)\n",
    "        dists = torch.cdist(flat_climbs, offset_manifold)\n",
    "        idx = dists.argmin(dim=1)\n",
    "        holds = self.holds_lookup[idx]\n",
    "        holds = holds.reshape(B, S)\n",
    "\n",
    "        print(type(holds),type(roles))\n",
    "        \n",
    "        # Mask null holds to be role 4\n",
    "        is_null = (holds == -1)\n",
    "        roles[is_null] = 4\n",
    "        \n",
    "        # Concatenate indices and roles\n",
    "        climbs = np.stack([holds, roles], axis=2)\n",
    "        \n",
    "        # Convert climbs into list[np.array] filtering on role != 4 \n",
    "        climbs = [c[c[:,1] != 4].tolist() for c in climbs]\n",
    "        \n",
    "        return climbs\n",
    "    \n",
    "    def _projection_strength(self, t: Tensor, t_start_projection: float = 0.8):\n",
    "        \"\"\"Calculate the weight to assign to the projected holds based on the timestep.\"\"\"\n",
    "        a = (t_start_projection-t)/t_start_projection\n",
    "        strength = 1 - torch.cos(a*torch.pi/2)\n",
    "        return torch.where(t > t_start_projection, torch.zeros_like(t), strength).unsqueeze(2)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        n: int = 1 ,\n",
    "        angle: int = 45,\n",
    "        grade: str = 'V4',\n",
    "        diff_scale: str = 'v_grade',\n",
    "        deterministic: bool = False\n",
    "    )->list[list[int]]:\n",
    "        \"\"\"\n",
    "        Generate a climb or batch of climbs with the given conditions using the standard DDPM iterative denoising process.\n",
    "        \n",
    "        :param n: Number of climbs to generate\n",
    "        :type n: int\n",
    "        :param angle: Angle of the wall\n",
    "        :type angle: int\n",
    "        :param grade: Desired difficulty (V-grade)\n",
    "        :type grade: int | None\n",
    "        :return: A Tensor containing the denoised generated climbs as hold sets.\n",
    "        :rtype: Tensor\n",
    "        \"\"\"\n",
    "        cond_t = self._build_cond_tensor(n, grade, diff_scale, angle)\n",
    "        x_t = torch.randn((n, 20, 4), device=self.device)\n",
    "        noisy = x_t.clone()\n",
    "        t_tensor = torch.ones((n,1), device=self.device)\n",
    "        \n",
    "        # Randomly offset the holds-manifold to allow for climbs to be generated at different x-coordinates around the wall.\n",
    "        x_offset = np.random.randn()\n",
    "        offset_manifold = self.holds_manifold.clone()\n",
    "        offset_manifold[:,0] += x_offset*0.1\n",
    "\n",
    "        for t in range(0, self.timesteps):\n",
    "            print('.',end='')\n",
    "\n",
    "            gen_climbs = self.ddpm(noisy, cond_t, t_tensor)\n",
    "\n",
    "            alpha_p = self._projection_strength(t_tensor)\n",
    "            projected_climbs = self._project_onto_manifold(gen_climbs, offset_manifold)\n",
    "            print(projected_climbs.shape, alpha_p.shape)\n",
    "            gen_climbs = alpha_p*(projected_climbs) + (1-alpha_p)*(gen_climbs)\n",
    "            \n",
    "            t_tensor -= 1.0/self.timesteps\n",
    "            noisy = self.ddpm.forward_diffusion(gen_climbs, t_tensor, x_t if deterministic else torch.randn_like(x_t))\n",
    "        \n",
    "        return self._project_onto_indices(gen_climbs, cond_t, offset_manifold)\n",
    "\n",
    "ddpm = ClimbDDPM(\n",
    "    model=Noiser(),\n",
    "    weights_path=DDPM_WEIGHTS_PATH,\n",
    "    timesteps=100,\n",
    ")\n",
    "scaler = ClimbsFeatureScaler(\n",
    "    weights_path=SCALER_WEIGHTS_PATH\n",
    ")\n",
    "hold_classifier = UNetHoldClassifierLogits(\n",
    "    weights_path=HC_WEIGHTS_PATH\n",
    ")\n",
    "generator = ClimbDDPMGenerator(\n",
    "    wall_id=WALL_ID,\n",
    "    scaler=scaler,\n",
    "    ddpm=ddpm,\n",
    "    hold_classifier=hold_classifier\n",
    ")\n",
    "climbs = generator.generate(n=2)\n",
    "for c in climbs:\n",
    "    print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
