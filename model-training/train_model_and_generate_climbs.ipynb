{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58147a07-9853-4be4-a328-294b4be4ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from climb_mlp_utils import load_and_preprocess_data, train_climb_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43414d07-30d0-4982-8059-38be163fe4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Constants ---\n",
    "PATH = 'data/all-data.json'\n",
    "NUM_LIMBS = 2\n",
    "FEATURE_DIM = 5\n",
    "INPUT_DIM = NUM_LIMBS * FEATURE_DIM     # 2 hands × 5 features\n",
    "OUTPUT_DIM = NUM_LIMBS * FEATURE_DIM    # Next position in Feature space\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "NULL_FEATURES = [-1.0, -1.0, 0.0, 0.0, -1.0]\n",
    "\n",
    "# --- Augmentation & Math Utilities ---\n",
    "\n",
    "def mirror_climb(sequence: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mirror a climb left-to-right. Swaps limbs and flips x-coordinates.\n",
    "    Expects sequence shape (N, 10).\n",
    "    \"\"\"\n",
    "    mirrored = sequence.copy()\n",
    "    \n",
    "    # 1. Swap LH (cols 0-4) and RH (cols 5-9)\n",
    "    mirrored[:, [0, 1, 2, 3, 4]] = sequence[:, [5, 6, 7, 8, 9]]\n",
    "    mirrored[:, [5, 6, 7, 8, 9]] = sequence[:, [0, 1, 2, 3, 4]]\n",
    "    \n",
    "    # 2. Invert norm_x (1-x) and pull_x (-x) for valid holds\n",
    "    # norm_x is at offset 0, pull_x is at offset 2\n",
    "    for limb_start_idx in [0, 5]:\n",
    "        norm_x_idx = limb_start_idx\n",
    "        pull_x_idx = limb_start_idx + 2\n",
    "        \n",
    "        # Mask: limb is not NULL (norm_x != -1)\n",
    "        mask = mirrored[:, norm_x_idx] != -1\n",
    "        \n",
    "        mirrored[mask, norm_x_idx] = 1.0 - mirrored[mask, norm_x_idx]\n",
    "        mirrored[mask, pull_x_idx] = -mirrored[mask, pull_x_idx]\n",
    "        \n",
    "    return mirrored\n",
    "\n",
    "\n",
    "def translate_climb(sequence: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate shifted max-left and shifted max-right variants.\n",
    "    Ensures all x-coordinates stay within [0, 1].\n",
    "    \"\"\"\n",
    "    # 1. Identify all valid X coordinates across both limbs\n",
    "    lh_x = sequence[:, 0]\n",
    "    rh_x = sequence[:, 5]\n",
    "    \n",
    "    valid_x_values = np.concatenate([lh_x[lh_x != -1], rh_x[rh_x != -1]])\n",
    "    \n",
    "    if valid_x_values.size == 0:\n",
    "        return sequence.copy(), sequence.copy()\n",
    "\n",
    "    # 2. Calculate max allowable shifts\n",
    "    min_x = np.min(valid_x_values)\n",
    "    max_x = np.max(valid_x_values)\n",
    "    \n",
    "    shift_left_val = min_x          # Amount to subtract to touch left wall\n",
    "    shift_right_val = 1.0 - max_x   # Amount to add to touch right wall\n",
    "\n",
    "    # 3. Apply shifts\n",
    "    left_variant = sequence.copy()\n",
    "    right_variant = sequence.copy()\n",
    "\n",
    "    for limb_idx in [0, 5]:\n",
    "        mask = sequence[:, limb_idx] != -1\n",
    "        left_variant[mask, limb_idx] -= shift_left_val\n",
    "        right_variant[mask, limb_idx] += shift_right_val\n",
    "\n",
    "    return left_variant, right_variant\n",
    "\n",
    "\n",
    "def augment_sequence_list(sequences: List[np.ndarray]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Expands a list of sequences by 6x using mirroring and translation.\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    for seq in sequences:\n",
    "        # Generate mirrored variation\n",
    "        mirrored = mirror_climb(seq)\n",
    "        \n",
    "        # Generate translations\n",
    "        orig_l, orig_r = translate_climb(seq)\n",
    "        mir_l, mir_r = translate_climb(mirrored)\n",
    "        \n",
    "        # Add all 6 variations\n",
    "        augmented.extend([seq, orig_l, orig_r, mirrored, mir_l, mir_r])\n",
    "        \n",
    "    return augmented\n",
    "\n",
    "\n",
    "def extract_hold_features(hold_data: dict) -> List[float]:\n",
    "    \"\"\"Extract normalized 5D feature vector from hold data dict.\"\"\"\n",
    "    if hold_data == -1:\n",
    "        return list(NULL_FEATURES)\n",
    "    \n",
    "    return [\n",
    "        float(hold_data['norm_x']),\n",
    "        float(hold_data['norm_y']),\n",
    "        float(hold_data['pull_x']),\n",
    "        float(hold_data['pull_y']),\n",
    "        float(hold_data['useability']) / 10.0\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_climb_to_numpy(climb_data: dict, hold_map: dict[int, List[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a raw climb dict into a (T, 10) numpy array of features.\n",
    "    \"\"\"\n",
    "    sequence_features = []\n",
    "    \n",
    "    for position in climb_data['sequence']:\n",
    "        feature_list=[]\n",
    "        for hold_idx in position:\n",
    "            if hold_idx == -1:\n",
    "                feature_list.append(NULL_FEATURES)\n",
    "            else:\n",
    "                feature_list.append(hold_map[hold_idx])\n",
    "        \n",
    "        # Combine features (Left + Right)\n",
    "        sequence_features.append(feature_list[0] + feature_list[1])\n",
    "        \n",
    "    return np.array(sequence_features, dtype=np.float32)\n",
    "\n",
    "def parse_moveset_to_numpy(moveset: dict, hold_map: dict[int, List[float]]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts a moveset JSON into a list of sequence pairs.\n",
    "    \"\"\"\n",
    "    moves = []\n",
    "    lh_start = hold_map[moveset[\"lh_start\"]]\n",
    "    rh_start = hold_map[moveset[\"rh_start\"]]\n",
    "\n",
    "    for h in moveset['lh_finish']:\n",
    "        lh_end = hold_map[h]\n",
    "        moves.append(np.array([lh_start+rh_start,lh_end+rh_start],dtype=np.float32))\n",
    "    for h in moveset['rh_finish']:\n",
    "        rh_end = hold_map[h]\n",
    "        moves.append(np.array([lh_start+rh_start,lh_start+rh_end],dtype=np.float32))\n",
    "\n",
    "    return moves\n",
    "# --- Dataset ---\n",
    "class ClimbDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of (current_hands, next_hold) pairs.\n",
    "    Accepts a list of pre-processed (potentially augmented) numpy sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: List[np.ndarray]):\n",
    "        self.examples = []\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # seq shape is (T, 10)\n",
    "            # Create pairs: Input(t) -> Target(t+1)\n",
    "            for t in range(len(seq) - 1):\n",
    "                input_feat = seq[t]\n",
    "                target_feat = seq[t + 1]\n",
    "                \n",
    "                self.examples.append((\n",
    "                    torch.tensor(input_feat, dtype=torch.float32),\n",
    "                    torch.tensor(target_feat, dtype=torch.float32)\n",
    "                ))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "# --- Data Pipeline ---\n",
    "def load_and_preprocess_data(path: str, val_split: float = 0.2) -> Tuple[ClimbDataset, ClimbDataset, dict[int, List[float]]]:\n",
    "    \"\"\"\n",
    "    Loads JSON, converts to numpy, splits data, augments TRAINING set only,\n",
    "    and returns ClimbDatasets.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {path}...\")\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    hold_map = {h['hold_id']: extract_hold_features(h) for h in data['holds']}\n",
    "    all_sequences = []\n",
    "    print(f\"Extracted {len(hold_map)} holds...\")\n",
    "\n",
    "    all_sequences.extend(augment_sequence_list([parse_climb_to_numpy(s,hold_map) for s in data['sequences']]))\n",
    "    all_sequences.extend([seq for moveset in data['movesets'] for seq in parse_moveset_to_numpy(moveset,hold_map)])\n",
    "    num_sequences = len(all_sequences)\n",
    "    print(f\"Extracted {num_sequences} sequences...\")\n",
    "    print(f\"{int(data['metadata']['num_moves']) * 6} training moves estimated with dataset augmentation...\")\n",
    "\n",
    "    # 2. Split indices\n",
    "    indices = np.arange(num_sequences)\n",
    "    np.random.shuffle(indices)\n",
    "    split_idx = int(num_sequences * (1 - val_split))\n",
    "    \n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    \n",
    "    train_seqs = [all_sequences[i] for i in train_indices]\n",
    "    val_seqs = [all_sequences[i] for i in val_indices]\n",
    "    \n",
    "    print(f\"Split: {len(train_seqs)} Train / {len(val_seqs)} Val\")\n",
    "    \n",
    "    # 4. Create Datasets\n",
    "    train_dataset = ClimbDataset(train_seqs)\n",
    "    val_dataset = ClimbDataset(val_seqs)\n",
    "    \n",
    "    return train_dataset, val_dataset, hold_map\n",
    "\n",
    "\n",
    "# --- Model ---\n",
    "class ClimbMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int = INPUT_DIM,\n",
    "                 hidden_dim: int = HIDDEN_DIM,\n",
    "                 output_dim: int = OUTPUT_DIM):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader, criterion: nn.Module, \n",
    "    optimizer: optim.Optimizer | None,\n",
    "    device: str\n",
    ") -> Tuple[float, float]:\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "    \n",
    "    total_loss, total_dist, n_samples = 0.0, 0.0, 0\n",
    "    \n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(inputs)\n",
    "            # Euclidean distance for monitoring\n",
    "            total_dist += torch.norm(outputs - targets, dim=1).sum().item()\n",
    "            n_samples += len(inputs)\n",
    "\n",
    "    return total_loss / n_samples, total_dist / n_samples\n",
    "\n",
    "\n",
    "# --- Inference ---\n",
    "class ClimbGenerator:\n",
    "    \"\"\"Generate climb sequences by predicting holds in feature space.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: ClimbMLP, hold_map: dict[int, List[float]], device: str):\n",
    "        self.model = model.to(device).eval()\n",
    "        self.hold_map = hold_map\n",
    "        self.device = device\n",
    "\n",
    "    def _to_input(self, lh: List[float], rh: List[float]) -> torch.Tensor:\n",
    "        \"\"\"Convert hand features to model input tensor.\"\"\"\n",
    "        return torch.tensor(lh + rh, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "\n",
    "    def _nearest_hold(self, features: np.ndarray) -> Tuple[int, List[float], int]:\n",
    "        \"\"\"Find hold with minimum Euclidean distance to predicted features.\"\"\"\n",
    "        best_id, best_dist = -1, float('inf')\n",
    "        \n",
    "        for hold_id, hold_features in self.hold_map.items():\n",
    "            dist = np.linalg.norm(features - np.array(hold_features))\n",
    "            if dist < best_dist:\n",
    "                best_id, best_dist = hold_id, dist\n",
    "                \n",
    "        return best_id, list(self.hold_map.get(best_id, NULL_FEATURES)), best_dist\n",
    "\n",
    "    def generate(self, \n",
    "                 start_lh: int, \n",
    "                 start_rh: int, \n",
    "                 max_moves: int = 10) -> List[Tuple[int, int, List[float], List[float]]]:\n",
    "        \"\"\"\n",
    "        Generate a climb sequence from starting hold indices.\n",
    "        \n",
    "        Returns list of (lh_id, rh_id, lh_features, rh_features) tuples.\n",
    "        \"\"\"\n",
    "        lh_id = start_lh\n",
    "        rh_id = start_rh\n",
    "        lh = self.hold_map[start_lh]\n",
    "        rh = self.hold_map[start_rh]\n",
    "        sequence = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for move in range(max_moves):\n",
    "                inputs = self._to_input(lh, rh)\n",
    "                predicted = self.model(inputs).cpu().numpy().flatten()\n",
    "                lh_id_next, lh_next, lh_dist = self._nearest_hold(predicted[:5])\n",
    "                rh_id_next, rh_next, rh_dist = self._nearest_hold(predicted[5:])\n",
    "\n",
    "                if lh_id_next != lh_id and rh_id_next != rh_id:\n",
    "                    if lh_dist < rh_dist:\n",
    "                        lh_id = lh_id_next\n",
    "                        lh = lh_next\n",
    "                    else:\n",
    "                        rh_id = rh_id_next\n",
    "                        rh = rh_next\n",
    "                else:\n",
    "                    lh, rh = lh_next, rh_next\n",
    "                    lh_id, rh_id = lh_id_next, rh_id_next\n",
    "                \n",
    "                if (len(sequence) > 0 and (lh_id, rh_id) == sequence[-1]) or (lh == NULL_FEATURES and rh == NULL_FEATURES):\n",
    "                    break\n",
    "\n",
    "                sequence.append((lh_id, rh_id))\n",
    "                    \n",
    "        return sequence\n",
    "\n",
    "\n",
    "def train_climb_generator(\n",
    "    train_ds: Dataset,\n",
    "    val_ds: Dataset,\n",
    "    hold_map: dict[int,List[float]],\n",
    "    num_epochs: int = 100,\n",
    "    lr: float = 0.001, \n",
    "    batch_size: int = 32,\n",
    "    device: str = \"cpu\"\n",
    "    ) -> ClimbGenerator:\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = ClimbMLP().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training\")\n",
    "    for _ in pbar:\n",
    "        train_loss, train_dist = run_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_dist = run_epoch(model, val_loader, criterion, None, device)\n",
    "        \n",
    "        pbar.set_postfix({\"T_MSE\": f\"{train_loss:.4f}\", \"V_MSE\": f\"{val_loss:.4f}\"})\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_climb_mlp.pth')\n",
    "            \n",
    "    return ClimbGenerator(model, hold_map, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215fcb01-6b83-4cd6-a1b6-e8fa17901113",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_JSON_PATH = 'data/all-data.json'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafac360-2e05-4b2b-bce1-ee4e8a2d165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/all-data.json...\n",
      "Extracted 263 holds...\n",
      "Extracted 554 sequences...\n",
      "2862 training moves estimated with dataset augmentation...\n",
      "Split: 443 Train / 111 Val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██| 500/500 [01:42<00:00,  4.87it/s, T_MSE=0.0114, V_MSE=0.0307]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data with Augmentation\n",
    "train_ds, val_ds, hold_map = load_and_preprocess_data(DATA_JSON_PATH, val_split=0.2)\n",
    "# 2. Train Model\n",
    "generator = train_climb_generator(train_ds, val_ds, hold_map, num_epochs=500, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f6b47e-12df-4db6-bcf3-027185ceba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 178\n",
      "[178, 178] [(178, 139), (82, 139), (82, 28), (82, 17), (6, 17), (140, 17), (50, 17), (16, 17), (195, 17), (5, 17)]\n",
      "206 139\n",
      "[206, 139] [(82, 139), (82, 28), (82, 17), (6, 17), (140, 17), (50, 17), (16, 17), (195, 17), (5, 17), (5, 6)]\n",
      "193 193\n",
      "[193, 193] [(193, 17), (28, 17)]\n",
      "176 187\n",
      "[176, 187] [(90, 187), (90, 88), (95, 88), (19, 88), (19, 52), (19, 20), (20, 20), (80, 20), (30, 20), (20, 20)]\n",
      "148 140\n",
      "[148, 140] [(148, 148), (148, 160), (144, 160), (140, 160), (144, 160), (140, 160), (144, 160), (140, 160), (144, 160), (140, 160)]\n",
      "176 176\n",
      "[176, 176] [(174, 176), (174, 110), (174, 144), (91, 144), (140, 144), (86, 144), (86, 79), (86, 53), (78, 53), (29, 53)]\n",
      "169 145\n",
      "[169, 145] [(169, 43), (169, 31), (48, 31), (32, 31), (32, 43)]\n",
      "177 173\n",
      "[177, 173] [(173, 173), (173, 45), (2, 45), (2, 19), (12, 19), (12, 14), (14, 14), (14, 83), (193, 83), (193, 161)]\n",
      "179 179\n",
      "[179, 179] [(179, 150), (144, 150)]\n",
      "181 172\n",
      "[181, 172] [(66, 172), (66, 22)]\n"
     ]
    }
   ],
   "source": [
    "climbs_to_watch = [[178,178],[206,139],[193,193],[176,187],[148,140],[176,176],[169,145],[177,173],[179,179],[181,172]]\n",
    "\n",
    "for climb in climbs_to_watch:\n",
    "    print(climb[0],climb[1])\n",
    "    print(climb, generator.generate(climb[0],climb[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "49406672-93ae-4652-94a3-d49223bca200",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ClimbGenerator(generator.model,generator.hold_map,generator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b467be2c-018c-40f1-bc60-5296216604e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(74, 90), (74, 19), (90, 19), (90, 38), (38, 38), (75, 38), (75, 45)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate(74,160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc71ac8-ea92-4c8b-b6b3-2fa909db6822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
