{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f04d26-b0f0-46c9-a9ef-b2cb36010bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torchinfo import summary\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "from simple_diffusion import Noiser, DDPMTrainer, ClimbDDPM, ClimbDDPMGenerator, clear_compile_keys\n",
    "from climb_conversion import ClimbsFeatureArray, ClimbsFeatureScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bcbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in ClimbDDPM(model=Noiser(hidden_dim=128, layers=5)).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test single-batch memorization to ensure both model architectures are working properly.\n",
    "def test_single_batch(model: nn.Module, dataset: TensorDataset, steps: int = 1000, lr = 1e-3, decay=0.0):\n",
    "    if decay == 0:\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(params = model.parameters(), lr=lr, weight_decay=decay)\n",
    "\n",
    "    loader = DataLoader(dataset=dataset, batch_size=64)\n",
    "    x, c = next(iter(loader))\n",
    "    losses = []\n",
    "    with tqdm(range(steps)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, c)\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            if epoch % 10 == 0:\n",
    "                pbar.set_postfix_str(f\"Loss: {loss.item():.4f}, Improvement:{losses[-1]-loss.item():.4f}, Grad Norm:{grad_norm:.4f}, Min loss:{min(losses) if len(losses) > 0 else 0:.5f}\")\n",
    "    print(f\"Min loss:{min(losses):.5f}\")\n",
    "    return losses\n",
    "\n",
    "def moving_average(values, window, gaussian = False):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    #We create the vector to multiply each value by to get the moving average. Essentially a vector of length n\n",
    "    # in which each weight is 1/n.\n",
    "    kernel = np.repeat(1.0, window) / window\n",
    "    if (gaussian == True) :\n",
    "        if window % 2 == 0:\n",
    "            window+=1\n",
    "        x = np.arange(-(window // 2), window // 2 + 1)\n",
    "        kernel = np.exp(-(x ** 2) / (2 * window ** 2))\n",
    "        kernel = kernel / np.sum(kernel)\n",
    "    \n",
    "    #The convolve function iteratively multiplies the first n values in the values array by the weights array.\n",
    "    # with the given weights array, it essentially takes the moving average of each N values in the values array.\n",
    "    return np.convolve(values, kernel, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d596f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1 = test_single_batch(ClimbDDPM(Noiser(), predict_noise=True),TensorDataset(*dataset))\n",
    "losses2 = test_single_batch(ClimbDDPM(ImprovedNoiser(128), predict_noise=True),TensorDataset(*dataset))\n",
    "\n",
    "#With LR Scheduler\n",
    "l1 = moving_average(losses1, 50)\n",
    "l2 = moving_average(losses2, 50)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(list(range(len(l1))), l1, label=f\"Losses U-Net{sum([p.numel() for p in Noiser().parameters()])}\")\n",
    "ax.plot(list(range(len(l2))), l2, label=f\"Losses Self-Attention({sum([p.numel() for p in ImprovedNoiser(128).parameters()])})\")\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d987184",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1 = test_single_batch(ClimbDDPM(Noiser(sinusoidal=True), predict_noise=True),TensorDataset(*dataset))\n",
    "losses2 = test_single_batch(ClimbDDPM(Noiser(), predict_noise=True),TensorDataset(*dataset))\n",
    "\n",
    "#With LR Scheduler\n",
    "l1 = moving_average(losses1, 50)\n",
    "l2 = moving_average(losses2, 50)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(list(range(len(l1))),l1, label=\"Losses Sinusoidal Emb\")\n",
    "ax.plot(list(range(len(l2))), l2, label=\"Losses Linear Emb\")\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1 = test_single_batch(ClimbDDPM(Noiser(), predict_noise=True),TensorDataset(*dataset))\n",
    "losses2 = test_single_batch(ClimbDDPM(ImprovedNoiser(), predict_noise=True),TensorDataset(*dataset), decay=1e-4)\n",
    "\n",
    "# Without LR Scheduler\n",
    "l1 = moving_average(np.array(losses1), 50)\n",
    "l2 = moving_average(np.array(losses2), 50)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(list(range(len(l1))), l1, label=f\"Losses No Weight Decay\")\n",
    "ax.plot(list(range(len(l2))), l2, label=f\"Losses Weight Decay\")\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c82598",
   "metadata": {},
   "outputs": [],
   "source": [
    "climbs = ClimbsFeatureArray('../data/storage.db')\n",
    "dataset = climbs.get_features(limit = 10000, augment_reflections=False)\n",
    "losses1 = test_single_batch(ClimbDDPM(Noiser(sinusoidal=False), predict_noise=True),TensorDataset(*dataset))\n",
    "losses2 = test_single_batch(ClimbDDPM(Noiser(), predict_noise=True),TensorDataset(*dataset))\n",
    "\n",
    "# Without LR Scheduler\n",
    "l1 = moving_average(np.array(losses1), 50)\n",
    "l2 = moving_average(np.array(losses2), 50)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(list(range(len(l1))), l1, label=f\"Losses w. Weight Decay ({sum([p.numel() for p in Noiser().parameters()])} params)\")\n",
    "ax.plot(list(range(len(l2))), l2, label=f\"Losses w0. Weight Decay (lr=.001, {sum([p.numel() for p in Noiser().parameters()])} params)\")\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sinusoidal Embedding on a medium-sized dataset.\n",
    "climbs = ClimbsFeatureArray('../data/storage.db')\n",
    "dataset = climbs.get_features(limit = 2000, augment_reflections=True)\n",
    "losses1 = test_single_batch(ClimbDDPM(Noiser(128), predict_noise=True),TensorDataset(*dataset), steps=3000)\n",
    "losses2 = test_single_batch(ClimbDDPM(Noiser(128, sinusoidal=True), predict_noise=True),TensorDataset(*dataset), steps=3000)\n",
    "\n",
    "# Without LR Scheduler\n",
    "l1 = moving_average(np.array(losses1), 50)\n",
    "l2 = moving_average(np.array(losses2), 50)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "\n",
    "ax.plot(list(range(len(l1))), l1, label=f\"Losses w. Linear Emb ({sum([p.numel() for p in Noiser(128).parameters()])} params)\")\n",
    "ax.plot(list(range(len(l2))), l2, label=f\"Losses w. Sinusoidal Emb ({sum([p.numel() for p in Noiser(128).parameters()])} params)\")\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a014f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "606921\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m([p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters()]))\n\u001b[32m     70\u001b[39m optimizer = torch.optim.Adam(model.parameters())\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m batches = DataLoader(\u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m)\u001b[49m, batch_size=\u001b[32m64\u001b[39m)\n\u001b[32m     73\u001b[39m losses = [[], [], []]\n\u001b[32m     74\u001b[39m x, c = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batches))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:205\u001b[39m, in \u001b[36mTensorDataset.__init__\u001b[39m\u001b[34m(self, *tensors)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *tensors: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mSize mismatch between tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = tensors\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:206\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *tensors: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m(\u001b[32m0\u001b[39m) == tensor.size(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[32m    207\u001b[39m     ), \u001b[33m\"\u001b[39m\u001b[33mSize mismatch between tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = tensors\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "class ClimbDASD(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, layers=3, sinusoidal=False):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.timesteps = 100\n",
    "        self.model = Noiser(hidden_dim,layers, feature_dim=9, sinusoidal=sinusoidal)\n",
    "        self.roles_head = nn.Softmax(dim=2)\n",
    "\n",
    "    def _cos_alpha_bar(self, t: Tensor)-> Tensor:\n",
    "        t = t.view(-1,1,1)\n",
    "        epsilon = 0.0004\n",
    "        return  torch.cos((t+epsilon)/(1+2*epsilon)*torch.pi/2)**2\n",
    "        \n",
    "    def predict(self, noisy, cond, t):\n",
    "        \"\"\"Return prediction for noise.\"\"\"\n",
    "        pred = self.model(noisy, cond, t)\n",
    "        pred_noise = pred[:,:,:4]\n",
    "        pred_roles = self.discrete_features_head(pred[:,:,4:])\n",
    "\n",
    "        a = self._cos_alpha_bar(t)\n",
    "\n",
    "        pred_clean_cont = (noisy - torch.sqrt(1-a)*pred_noise)/torch.sqrt(a)\n",
    "\n",
    "        return pred_clean_cont, pred_roles\n",
    "    \n",
    "    def loss(self, clean, cond):\n",
    "        \"\"\"Get the model's loss from training on a dataset of clean (denoised) data.\"\"\"\n",
    "        B, H, S = clean.shape\n",
    "        t = torch.round(torch.rand(B, 1, device=self.device), decimals=2)\n",
    "        x_0 = torch.randn((B, H, S-5))\n",
    "        noisy = self.forward_diffusion(clean, t, x_0)\n",
    "\n",
    "        pred = self.model(noisy, cond, t)\n",
    "        pred_roles = self.roles_head(pred[:,:,4:])\n",
    "        continuous_loss = F.mse_loss(pred[:,:,:4], x_0[:,:,:4])\n",
    "        discrete_loss = F.cross_entropy(pred_roles, clean[:,:,4:])\n",
    "\n",
    "        return continuous_loss + discrete_loss, continuous_loss, discrete_loss\n",
    "    \n",
    "    def forward_diffusion(self, clean, t, x_0: Tensor):\n",
    "        \"\"\"\n",
    "        Perform the forward Diffusion process in two stages:\n",
    "            *Continuous Diffusion over Continuous Features [0:4]\n",
    "            *Discrete Absorbing State Diffusion over Discrete Features [4:9]\n",
    "        \n",
    "        :param clean: Full feature set (4 continuous, 5 discrete roles OH-Encoded)\n",
    "        :param t: Timestep Tensor\n",
    "        :param x_0: Optionally include the tensor for the prior 'full-noise' array to use instead of random noise. This makes the generation process deterministic.\n",
    "        :return: Diffused climbs, conditioned on timestep\n",
    "        :rtype: Tensor\n",
    "        \"\"\"\n",
    "        cont_feat = clean[:,:,:4]\n",
    "        disc_feat = clean[:,:,4:]\n",
    "        a = self._cos_alpha_bar(t)\n",
    "\n",
    "        diff_cont = torch.sqrt(a)*cont_feat + torch.sqrt(1-a)*(x_0)\n",
    "\n",
    "        das_mask = (a > torch.rand_like(a)).float()\n",
    "        diff_disc = disc_feat*das_mask\n",
    "\n",
    "        return torch.cat([diff_cont,diff_disc],dim=2)\n",
    "    \n",
    "    def forward(self, noisy, cond, t):\n",
    "        return self.predict(noisy, cond, t)\n",
    "\n",
    "climbs = ClimbsFeatureArray('../data/storage.db')\n",
    "full_dataset = climbs.get_features(limit=1000, continuous_only=False)\n",
    "model = ClimbDASD(64,2)\n",
    "print(sum([p.numel() for p in model.parameters()]))\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "batches = DataLoader(TensorDataset(*full_dataset), batch_size=64)\n",
    "losses = [[], [], []]\n",
    "x, c = next(iter(batches))\n",
    "\n",
    "with tqdm(range(1000)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        totals = [0, 0, 0]\n",
    "        optimizer.zero_grad()\n",
    "        loss, cont, disc = model.loss(x,c)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totals[0]+=loss.item()\n",
    "        totals[1]+=cont.item()\n",
    "        totals[2]+=disc.item()\n",
    "        pbar.set_postfix_str(f\"Epoch: {epoch}, Loss: {totals[0]:.2f}, Continuous: {totals[1]:.2f}, Discrete: {totals[2]:.2f}\")\n",
    "        for i in range(3):\n",
    "            losses[i].append(totals[i])\n",
    "\n",
    "l = moving_average(losses[0], 25)\n",
    "l1 = moving_average(losses[1], 25)\n",
    "l2 = moving_average(losses[2], 25)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.array(range(len(l))), y=l, ax=ax)\n",
    "sns.lineplot(x=np.array(range(len(l1))), y=l1, ax=ax)\n",
    "sns.lineplot(x=np.array(range(len(l2))), y=l2, ax=ax)\n",
    "ax.set_yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30629784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EvanM\\Documents\\Projects\\GitHub\\ml-homewall-climb-generator\\model-training\\equivariant_projected_diffusion\\simple_diffusion.py:613: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(filepath, map_location=map_loc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 384\u001b[39m\n\u001b[32m    374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._project_onto_indices(gen_climbs)\n\u001b[32m    376\u001b[39m generator = ClimbDDPMGenerator(\n\u001b[32m    377\u001b[39m     model = ClimbDDPM(model=Noiser()),\n\u001b[32m    378\u001b[39m     scaler = ClimbsFeatureScaler(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     scaler_weights_path = \u001b[33m'\u001b[39m\u001b[33mclimb_ddpm_scaler.joblib\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    383\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m climbs = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[38;5;28mprint\u001b[39m(climbs[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\EvanM\\anaconda3\\envs\\torch-cpu\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 374\u001b[39m, in \u001b[36mClimbDDPMGenerator.generate\u001b[39m\u001b[34m(self, n, angle, grade, diff_scale, deterministic, projected, show_steps, return_indices)\u001b[39m\n\u001b[32m    371\u001b[39m     t_tensor -= \u001b[32m1.0\u001b[39m/\u001b[38;5;28mself\u001b[39m.timesteps\n\u001b[32m    372\u001b[39m     noisy = \u001b[38;5;28mself\u001b[39m.model.forward_diffusion(gen_climbs, t_tensor, x_t \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;28;01melse\u001b[39;00m torch.randn_like(x_t))\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_project_onto_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_climbs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 316\u001b[39m, in \u001b[36mClimbDDPMGenerator._project_onto_indices\u001b[39m\u001b[34m(self, gen_climbs)\u001b[39m\n\u001b[32m    314\u001b[39m climbs = []\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gen_climb \u001b[38;5;129;01min\u001b[39;00m gen_climbs:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     flat_climb = gen_climb.reshape(-\u001b[32m1\u001b[39m,\u001b[43mH\u001b[49m)\n\u001b[32m    317\u001b[39m     dists = torch.cdist(flat_climb, \u001b[38;5;28mself\u001b[39m.holds_manifold)\n\u001b[32m    318\u001b[39m     idx = dists.argmin(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_climb(climb_data, title=\"Generated Climb\"):\n",
    "    \"\"\"\n",
    "    Visualizes a climb generated by a neural network.\n",
    "    \n",
    "    Args:\n",
    "        climb_data (np.array): Shape [20, 4]. \n",
    "                               Features: [x, y, pull_x, pull_y, role_emb]\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # role = np.argmax(climb_data[:,5:], axis=1)\n",
    "    real_holds = climb_data[climb_data[:,0] > -1.1]\n",
    "    x, y, pull_x, pull_y = real_holds.T\n",
    "\n",
    "    # 2. Setup the Plot\n",
    "    # Climbing walls are vertical, so we use a tall figsize\n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    \n",
    "    # 3. Plot Hand Holds (Circles, Blue)\n",
    "    # We filter using the inverted boolean mask\n",
    "    ax.scatter(x, y, c='blue', s=90)\n",
    "\n",
    "    # 5. Plot Pull Vectors (Arrows)\n",
    "    # quiver(x, y, u, v) plots arrows at (x,y) with direction (u,v)\n",
    "    ax.quiver(x, y, pull_x, pull_y, \n",
    "              color='green', alpha=0.6, \n",
    "              angles='xy', scale_units='xy', scale=1, \n",
    "              width=0.005, headwidth=4,\n",
    "              label='Pull Direction', zorder=1)\n",
    "\n",
    "    # 6. Formatting\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"X Position (Normalized)\")\n",
    "    ax.set_ylabel(\"Y Position (Normalized)\")\n",
    "    ax.set_ylim(-1,1)\n",
    "    ax.set_xlim(-1,1)\n",
    "    \n",
    "    # Important: set aspect to 'equal' so the wall doesn't look stretched\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard sinusoidal positional embeddings for time steps.\n",
    "    Helps the model understand 'where' it is in the diffusion process.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        # time: (batch_size, 1)\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, 0].unsqueeze(1) * embeddings.unsqueeze(0)\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cond_dim, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=padding)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.cond_proj = nn.Linear(cond_dim, out_channels*2)\n",
    "        self.shortcut = nn.Conv1d(in_channels,out_channels,1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "\n",
    "        gamma, beta = self.cond_proj(cond).unsqueeze(-1).chunk(2, dim=1)\n",
    "        h = h*(1+gamma) + beta\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "class Noiser(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, layers = 5, feature_dim = 4, cond_dim = 4, sinusoidal = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            (SinusoidalPositionEmbeddings(hidden_dim) if sinusoidal else nn.Linear(1,hidden_dim)),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(cond_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.combine_t_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.init_conv = ResidualBlock1D(feature_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "        self.down_blocks = nn.ModuleList([ResidualBlock1D(hidden_dim*(i+1), hidden_dim*(i+2), hidden_dim) for i in range(layers)])\n",
    "        self.up_blocks = nn.ModuleList([ResidualBlock1D(hidden_dim*(i+1), hidden_dim*(i), hidden_dim) for i in range(layers,0,-1)])\n",
    "\n",
    "        self.head = nn.Conv1d(hidden_dim, feature_dim, 1)\n",
    "    \n",
    "    def forward(self, climbs: Tensor, cond: Tensor, t: Tensor)-> Tensor:\n",
    "        \"\"\"\n",
    "        Run denoising pass. Predicts the added noise from the noisy data.\n",
    "        \n",
    "        :param climbs: Tensor with hold-set positions. [B, S, 4]\n",
    "        :param cond: Tensor with conditional variables. [B, 4]\n",
    "        :param t: Tensor with timestep of diffusion. [B, 1]\n",
    "        \"\"\"\n",
    "        emb_t = self.time_mlp(t)\n",
    "        emb_c = self.cond_mlp(cond)\n",
    "        emb_c = self.combine_t_mlp(torch.cat([emb_t, emb_c],dim=1))\n",
    "        emb_h = self.init_conv(climbs.transpose(1,2), emb_c)\n",
    "        \n",
    "        residuals = []\n",
    "        for layer in self.down_blocks:\n",
    "            residuals.append(emb_h)\n",
    "            emb_h = layer(emb_h, emb_c)\n",
    "        \n",
    "        for layer in self.up_blocks:\n",
    "            residual = residuals.pop()\n",
    "            emb_h = residual + layer(emb_h, emb_c)\n",
    "        \n",
    "        result = self.head(emb_h).transpose(1,2)\n",
    "\n",
    "        return result\n",
    "\n",
    "class ClimbDDPM(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model\n",
    "        self.timesteps = 100\n",
    "    \n",
    "    def _cos_alpha_bar(self, t: Tensor)-> Tensor:\n",
    "        t = t.view(-1,1,1)\n",
    "        epsilon = 0.0004\n",
    "        return  torch.cos((t+epsilon)/(1+2*epsilon)*torch.pi/2)**2\n",
    "    \n",
    "    def loss(self, sample_climbs, cond):\n",
    "        \"\"\"Perform a diffusion Training step and return the loss resulting from the model in the training run. Currently returns tuple (loss, real_hold_loss, null_hold_loss)\"\"\"\n",
    "        B, S, H = sample_climbs.shape\n",
    "        t = torch.round(torch.rand(B, 1, device=self.device), decimals=2)\n",
    "        x_0 = torch.randn((B, S, H), device = self.device)\n",
    "        noisy = self.forward_diffusion(sample_climbs, t, x_0)\n",
    "        pred_x_0 = self.model(noisy, cond, t)\n",
    "        return F.mse_loss(pred_x_0, x_0)\n",
    "    \n",
    "    def predict_clean(self, noisy, cond, t):\n",
    "        \"\"\"Return predicted clean data.\"\"\"\n",
    "        a = self._cos_alpha_bar(t)\n",
    "        prediction = self.model(noisy, cond, t)\n",
    "        clean = (noisy - torch.sqrt(1-a)*prediction)/torch.sqrt(a)\n",
    "        return clean\n",
    "    \n",
    "    def forward_diffusion(self, clean: Tensor, t: Tensor, x_0: Tensor)-> Tensor:\n",
    "        \"\"\"Perform forward diffusion to add noise to clean data based on noise adding schedule.\"\"\"\n",
    "        a = self._cos_alpha_bar(t)\n",
    "        return torch.sqrt(a) * clean + torch.sqrt(1-a) * x_0\n",
    "    \n",
    "    def forward(self, noisy, cond, t):\n",
    "        return self.predict_clean(noisy, cond, t)\n",
    "\n",
    "class ClimbDDPMGenerator():\n",
    "    \"\"\"Moving Climb Generation logic over here to implement automatic conditional feature scaling. Need to implement Projected Diffusion.\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            wall_id: str,\n",
    "            db_path: str,\n",
    "            scaler: ClimbsFeatureScaler,\n",
    "            model: ClimbDDPM,\n",
    "            model_weights_path: str | None,\n",
    "            scaler_weights_path: str | None\n",
    "        ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.scaler = scaler\n",
    "        self.model = model\n",
    "        self.timesteps = self.model.timesteps\n",
    "\n",
    "        if model_weights_path:\n",
    "            model.load_state_dict(state_dict=clear_compile_keys(model_weights_path),strict=True)\n",
    "        if scaler_weights_path:\n",
    "            self.scaler.load_weights(scaler_weights_path)\n",
    "\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            holds = pd.read_sql_query(\"SELECT hold_index, x, y, pull_x, pull_y, useability, is_foot, wall_id FROM holds WHERE wall_id = ?\",conn,params=(wall_id,))\n",
    "            scaled_holds = self.scaler.transform_hold_features(holds, to_df=True)\n",
    "            self.holds_manifold = torch.tensor(scaled_holds[['x','y','pull_x','pull_y']].values, dtype=torch.float32)\n",
    "            self.holds_lookup = scaled_holds['hold_index'].values\n",
    "        \n",
    "        self.holds_lookup = np.concatenate([self.holds_lookup, np.array([-1, -1, -1, -1])])\n",
    "        \n",
    "        self.holds_manifold = torch.cat([\n",
    "            self.holds_manifold,\n",
    "            torch.tensor(\n",
    "                [[-2.0, 0.0, -2.0, 0.0],\n",
    "                [2.0, 0.0, -2.0, 0.0],\n",
    "                [-2.0, 0.0, 2.0, 0.0],\n",
    "                [2.0, 0.0, 2.0, 0.0]],dtype=torch.float32)\n",
    "            ],dim=0)\n",
    "\n",
    "        self.grade_to_diff = {\n",
    "            'font': {\n",
    "                '4a': 10,\n",
    "                '4b': 11,\n",
    "                '4c': 12,\n",
    "                '5a': 13,\n",
    "                '5b': 14,\n",
    "                '5c': 15,\n",
    "                '6a': 16,\n",
    "                '6a+': 17,\n",
    "                '6b': 18,\n",
    "                '6b+': 19,\n",
    "                '6c': 20,\n",
    "                '6c+': 21,\n",
    "                '7a': 22,\n",
    "                '7a+': 23,\n",
    "                '7b': 24,\n",
    "                '7b+': 25,\n",
    "                '7c': 26,\n",
    "                '7c+': 27,\n",
    "                '8a': 28,\n",
    "                '8a+': 29,\n",
    "                '8b': 30,\n",
    "                '8b+': 31,\n",
    "                '8c': 32,\n",
    "                '8c+': 33\n",
    "            }, \n",
    "            'v_grade': {\n",
    "                'V0-': 10,\n",
    "                'V0': 11,\n",
    "                'V0+': 12,\n",
    "                'V1': 13,\n",
    "                'V1+': 14,\n",
    "                'V2': 15,\n",
    "                'V3': 16,\n",
    "                'V3+': 17,\n",
    "                'V4': 18,\n",
    "                'V4+': 19,\n",
    "                'V5': 20,\n",
    "                'V5+': 21,\n",
    "                'V6': 22,\n",
    "                'V6+': 22.5,\n",
    "                'V7': 23,\n",
    "                'V7+': 23.5,\n",
    "                'V8': 24,\n",
    "                'V8+': 25,\n",
    "                'V9': 26,\n",
    "                'V9+': 26.5,\n",
    "                'V10': 27,\n",
    "                'V10+': 27.5,\n",
    "                'V11': 28,\n",
    "                'V11+': 28.5,\n",
    "                'V12': 29,\n",
    "                'V12+': 29.5,\n",
    "                'V13': 30,\n",
    "                'V13+': 30.5,\n",
    "                'V14': 31,\n",
    "                'V14+': 31.5,\n",
    "                'V15': 32,\n",
    "                'V15+': 32.5,\n",
    "                'V16': 33\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _build_cond_tensor(self, n, grade, diff_scale, angle):\n",
    "        diff = self.grade_to_diff[diff_scale][grade]\n",
    "        df_cond = pd.DataFrame({\n",
    "            \"grade\": [diff]*n,\n",
    "            \"quality\": [2.9]*n,\n",
    "            \"ascents\": [100]*n,\n",
    "            \"angle\": [angle]*n\n",
    "        })\n",
    "\n",
    "        cond = self.scaler.transform_climb_features(df_cond).T\n",
    "        return torch.tensor(cond, device=self.device, dtype=torch.float32)\n",
    "    \n",
    "    def _project_onto_manifold(self, gen_climbs: Tensor)-> Tensor:\n",
    "        \"\"\"\n",
    "            Project each generated hold to its nearest neighbor on the hold manifold.\n",
    "            \n",
    "            Args:\n",
    "                gen_climbs: (B, S, H) predicted clean holds\n",
    "                return_indices: (boolean) Whether to return the hold indices or hold feature coordinates\n",
    "            Returns:\n",
    "                projected: (B, S, H) each hold snapped to nearest manifold point\n",
    "        \"\"\"\n",
    "        B, S, H = gen_climbs.shape\n",
    "        flat_climbs = gen_climbs.reshape(-1,H)\n",
    "        dists = torch.cdist(flat_climbs, self.holds_manifold)\n",
    "        idx = dists.argmin(dim=1)\n",
    "        return self.holds_manifold[idx].reshape(B, S, -1)\n",
    "        \n",
    "    def _project_onto_indices(self, gen_climbs: Tensor) -> list[list[int]]:\n",
    "        \"\"\"Project climb onto the final hold indices (and remove null holds)\"\"\"\n",
    "        \n",
    "        B, S, H = gen_climbs.shape\n",
    "\n",
    "        climbs = []\n",
    "        for gen_climb in gen_climbs:\n",
    "            flat_climb = gen_climb.reshape(-1,H)\n",
    "            dists = torch.cdist(flat_climb, self.holds_manifold)\n",
    "            idx = dists.argmin(dim=1)\n",
    "            idx = idx.detach().numpy()\n",
    "            holds = self.holds_lookup[idx]\n",
    "            climb = list(set(holds[holds > 0].tolist()))\n",
    "            climbs.append(climb)\n",
    "        \n",
    "        return climbs\n",
    "    \n",
    "    def _projection_strength(self, t: Tensor, t_start_projection: float = 0.5):\n",
    "        \"\"\"Calculate the weight to assign to the projected holds based on the timestep.\"\"\"\n",
    "        a = (t_start_projection-t)/t_start_projection\n",
    "        strength = 1 - torch.cos(a*torch.pi/2)\n",
    "        return torch.where(t > t_start_projection, torch.zeros_like(t), strength)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        n: int = 1 ,\n",
    "        angle: int = 45,\n",
    "        grade: str = 'V4',\n",
    "        diff_scale: str = 'v_grade',\n",
    "        deterministic: bool = False,\n",
    "        projected: bool = True,\n",
    "        show_steps: bool = False,\n",
    "        return_indices = True\n",
    "    )->list[list[int]]:\n",
    "        \"\"\"\n",
    "        Generate a climb or batch of climbs with the given conditions using the standard DDPM iterative denoising process.\n",
    "        \n",
    "        :param n: Number of climbs to generate\n",
    "        :type n: int\n",
    "        :param angle: Angle of the wall\n",
    "        :type angle: int\n",
    "        :param grade: Desired difficulty (V-grade)\n",
    "        :type grade: int | None\n",
    "        :return: A Tensor containing the denoised generated climbs as hold sets.\n",
    "        :rtype: Tensor\n",
    "        \"\"\"\n",
    "        cond_t = self._build_cond_tensor(n, grade, diff_scale, angle)\n",
    "        x_t = torch.randn((n, 20, 4), device=self.device)\n",
    "        noisy = x_t.clone()\n",
    "        t_tensor = torch.ones((n,1), device=self.device)\n",
    "\n",
    "        for t in range(0, self.timesteps):\n",
    "            print('.',end='')\n",
    "\n",
    "            gen_climbs = self.model(noisy, cond_t, t_tensor)\n",
    "\n",
    "            if projected:\n",
    "                alpha_p = self._projection_strength(t_tensor)\n",
    "                projected_climbs = self._project_onto_manifold(gen_climbs)\n",
    "                gen_climbs = alpha_p*(projected_climbs) + (1-alpha_p)*(gen_climbs)\n",
    "            \n",
    "            t_tensor -= 1.0/self.timesteps\n",
    "            noisy = self.model.forward_diffusion(gen_climbs, t_tensor, x_t if deterministic else torch.randn_like(x_t))\n",
    "        \n",
    "        return self._project_onto_indices(gen_climbs)\n",
    "\n",
    "generator = ClimbDDPMGenerator(\n",
    "    model = ClimbDDPM(model=Noiser()),\n",
    "    scaler = ClimbsFeatureScaler(),\n",
    "    wall_id = 'wall-0a877f13d8e5',\n",
    "    db_path = '../data/storage.db',\n",
    "    model_weights_path = 'simple-diffusion-large-2.pth',\n",
    "    scaler_weights_path = 'climb_ddpm_scaler.joblib'\n",
    ")\n",
    "climbs = generator.generate()\n",
    "print(climbs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fd304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimbsFeatureScaler:\n",
    "    def __init__(self, weights_path: str | None = None):\n",
    "        self.cond_features_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        self.hold_features_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        if weights_path and os.path.exists(weights_path):\n",
    "            self.load_weights(weights_path)\n",
    "    def save_weights(self, path: str):\n",
    "        \"\"\"Save weights to weights path.\"\"\"\n",
    "        state = {\n",
    "            'cond_scaler': self.cond_features_scaler,\n",
    "            'hold_scaler': self.hold_features_scaler\n",
    "        }\n",
    "        joblib.dump(state, path)\n",
    "    \n",
    "    def load_weights(self, path: str):\n",
    "        \"\"\"Load saved MinMaxScaler weights from the weights path.\"\"\"\n",
    "        state = joblib.load(path)\n",
    "        self.cond_features_scaler = state['cond_scaler']\n",
    "        self.hold_features_scaler = state['hold_scaler']\n",
    "        \n",
    "    def fit_transform(self, climbs_to_fit: pd.DataFrame, holds_to_fit: pd.DataFrame):\n",
    "        \"\"\"Function for fitting the MinMax scalers to the climbs and holds dataframes and returning the transformed climbs and holds df\"\"\"\n",
    "        # Fit preprocessing steps for climbs DF\n",
    "        scaled_climbs = climbs_to_fit.copy()\n",
    "        scaled_climbs = self._apply_log_transforms(scaled_climbs)\n",
    "        scaled_climbs[['grade','quality','ascents','angle']] = self.cond_features_scaler.fit_transform(scaled_climbs[['grade','quality','ascents','angle']])\n",
    "        # For holds DF\n",
    "        scaled_holds = self._apply_hold_transforms(holds_to_fit.copy())\n",
    "        scaled_holds[['x','y','pull_x','pull_y']] = self.hold_features_scaler.fit_transform(scaled_holds[['x','y','pull_x','pull_y']])\n",
    "        \n",
    "        return (scaled_climbs, scaled_holds)\n",
    "    \n",
    "    def _apply_log_transforms(self, dfc: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Covers Log transformation logic\"\"\"\n",
    "        dfc['quality'] -= 3\n",
    "        dfc['quality'] = np.log(1-dfc['quality'])\n",
    "        dfc['ascents'] = np.log(dfc['ascents'])\n",
    "\n",
    "        return dfc\n",
    "    \n",
    "    def _apply_hold_transforms(self, dfh: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Covers useability and is_foot embedding logic\"\"\"\n",
    "        dfh['mult'] = dfh['useability'] / ((3 * dfh['is_foot'])+1)\n",
    "        dfh['pull_x'] *= dfh['mult']\n",
    "        dfh['pull_y'] *= dfh['mult']\n",
    "        return dfh\n",
    "    \n",
    "    def transform_climb_features(self, dfc: pd.DataFrame, to_df: bool = False):\n",
    "        \"\"\"Turn a series of conditional climb features into normalized features for the DDPM.\"\"\"\n",
    "        dfc = self._apply_log_transforms(dfc)\n",
    "        if to_df:\n",
    "            dfc[['grade','quality','ascents','angle']] = self.cond_features_scaler.transform(dfc[['grade','quality','ascents','angle']])\n",
    "        else:\n",
    "            dfc = self.cond_features_scaler.transform(dfc[['grade','quality','ascents','angle']])\n",
    "\n",
    "        return dfc\n",
    "    \n",
    "    def transform_hold_features(self, dfh: pd.DataFrame, to_df:bool=False):\n",
    "        \"\"\"Turn a series of hold features into normalized features for the DDPM.\"\"\"\n",
    "        dfh = self._apply_hold_transforms(dfh)\n",
    "        if to_df:\n",
    "            dfh[['x','y','pull_x','pull_y']] = self.hold_features_scaler.transform(dfh[['x','y','pull_x','pull_y']])\n",
    "        else:\n",
    "            dfh = self.hold_features_scaler.transform(dfh[['x','y','pull_x','pull_y']])\n",
    "\n",
    "        return dfh\n",
    "\n",
    "weights_path = 'climb_ddpm_scaler.joblib'\n",
    "db_path = '../data/storage.db'\n",
    "to_length = 20\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    query = \"SELECT * FROM climbs WHERE ascents > 1\"\n",
    "    all_climbs = pd.read_sql_query(query, conn, index_col='id')\n",
    "    all_holds = pd.read_sql_query(\"SELECT hold_index, x, y, pull_x, pull_y, useability, is_foot, wall_id FROM holds\",conn)\n",
    "    scaler = ClimbsFeatureScaler()\n",
    "    scaled_climbs, scaled_holds = scaler.fit_transform(all_climbs,all_holds)\n",
    "    scaler.save_weights(weights_path)\n",
    "\n",
    "scaler = ClimbsFeatureScaler()\n",
    "scaler.load_weights(weights_path)\n",
    "rescaled_holds = scaler.transform_hold_features(all_holds[all_holds['wall_id']=='wall-443c15cd12e0'], to_df=False)\n",
    "rescaled_climbs = scaler.transform_climb_features(all_climbs, to_df=False)\n",
    "\n",
    "dfh = pd.DataFrame(rescaled_holds,columns=['x','y','pull_x','pull_y'])\n",
    "dfhs = pd.DataFrame(scaled_holds,columns=['x','y','pull_x','pull_y'])\n",
    "dfc = pd.DataFrame(rescaled_climbs,columns=['grade','quality','ascents','angle'])\n",
    "dfcs = pd.DataFrame(scaled_climbs,columns=['grade','quality','ascents','angle'])\n",
    "fig, axes = plt.subplots(4,4, figsize=(8,12))\n",
    "\n",
    "for i, c in enumerate(dfh.columns):\n",
    "    sns.kdeplot(dfh, x=c, ax=axes[0][i%4])\n",
    "for i, c in enumerate(dfhs.columns):\n",
    "    sns.kdeplot(dfhs, x=c, ax=axes[1][i%4])\n",
    "for i, c in enumerate(dfc.columns):\n",
    "    sns.kdeplot(dfc, x=c, ax=axes[2][i%4])\n",
    "for i, c in enumerate(dfcs.columns):\n",
    "    sns.kdeplot(dfcs, x=c, ax=axes[3][i%4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "df_cond = pd.DataFrame({\"grade\":[24]*n,\"quality\":[3]*n,\"ascents\":[1000]*n,\"angle\":[45]*n})\n",
    "scaler = ClimbsFeatureScaler(weights_path='climb_ddpm_scaler.joblib')\n",
    "scaled_cond = scaler.preprocess_conditional_features(df_cond)\n",
    "scaled_cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c46a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"../data/boardlib/tension.db\") as conn:\n",
    "    dfd = pd.read_sql_query(\"SELECT * FROM difficulty_grades WHERE is_listed = 1\",conn)\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_grades = []\n",
    "font_grades = []\n",
    "for l in dfd['boulder_name'].str.split('/'):\n",
    "    font_grades.append(l[0])\n",
    "    v_grades.append(l[1])\n",
    "\n",
    "font_to_diff = list(zip(v_grades,dfd['difficulty']))\n",
    "font_to_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f0a674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V0-': 10,\n",
       " 'V0': 11,\n",
       " 'V0+': 12,\n",
       " 'V1': 13,\n",
       " 'V1+': 14,\n",
       " 'V2': 15,\n",
       " 'V3': 16,\n",
       " 'V3+': 17,\n",
       " 'V4': 18,\n",
       " 'V4+': 19,\n",
       " 'V5': 20,\n",
       " 'V5+': 21,\n",
       " 'V6': 22,\n",
       " 'V6+': 22.5,\n",
       " 'V7': 23,\n",
       " 'V7+': 23.5,\n",
       " 'V8': 24,\n",
       " 'V8+': 25,\n",
       " 'V9': 26,\n",
       " 'V9+': 26.5,\n",
       " 'V10': 27,\n",
       " 'V10+': 27.5,\n",
       " 'V11': 28,\n",
       " 'V11+': 28.5,\n",
       " 'V12': 29,\n",
       " 'V12+': 29.5,\n",
       " 'V13': 30,\n",
       " 'V13+': 30.5,\n",
       " 'V14': 31,\n",
       " 'V14+': 31.5,\n",
       " 'V15': 32,\n",
       " 'V15+': 32.5,\n",
       " 'V16': 33}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict([('V0-', 10),\n",
    " ('V0', 11),\n",
    " ('V0+', 12),\n",
    " ('V1', 13),\n",
    " ('V1+', 14),\n",
    " ('V2', 15),\n",
    " ('V3', 16),\n",
    " ('V3+', 17),\n",
    " ('V4', 18),\n",
    " ('V4+', 19),\n",
    " ('V5', 20),\n",
    " ('V5+', 21),\n",
    " ('V6', 22),\n",
    " ('V6+', 22.5),\n",
    " ('V7', 23),\n",
    " ('V7+', 23.5),\n",
    " ('V8', 24),\n",
    " ('V8+', 25),\n",
    " ('V9', 26),\n",
    " ('V9+', 26.5),\n",
    " ('V10', 27),\n",
    " ('V10+', 27.5),\n",
    " ('V11', 28),\n",
    " ('V11+', 28.5),\n",
    " ('V12', 29),\n",
    " ('V12+', 29.5),\n",
    " ('V13', 30),\n",
    " ('V13+', 30.5),\n",
    " ('V14', 31),\n",
    " ('V14+', 31.5),\n",
    " ('V15', 32),\n",
    " ('V15+', 32.5),\n",
    " ('V16', 33)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
