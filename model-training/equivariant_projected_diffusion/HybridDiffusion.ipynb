{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f04d26-b0f0-46c9-a9ef-b2cb36010bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "from climb_conversion import ClimbsFeatureArray\n",
    "# climbs = ClimbsFeatureArray(db_path=\"../data/storage.db\").get_features_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cond_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        self.cond_proj = nn.Linear(cond_dim, out_channels*2)\n",
    "        self.shortcut = nn.Conv1d(in_channels,out_channels,1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "\n",
    "        gamma, beta = self.cond_proj(cond).unsqueeze(-1).chunk(2, dim=1)\n",
    "        h = h*(1+gamma) + beta\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        return h + self.shortcut(x)\n",
    "class ClimbingUNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=10, cond_dim=4, base_dim=64):\n",
    "        super().__init__()\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, base_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(base_dim, base_dim),\n",
    "        )\n",
    "        \n",
    "        # Input projection\n",
    "        self.init_conv = nn.Conv1d(in_channels, base_dim, 3, padding=1)\n",
    "        \n",
    "        # Downsample\n",
    "        self.down1 = ResidualBlock1D(base_dim, base_dim, base_dim + cond_dim)\n",
    "        self.down2 = ResidualBlock1D(base_dim, base_dim*2, base_dim + cond_dim)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.mid = ResidualBlock1D(base_dim*2, base_dim*2, base_dim + cond_dim)\n",
    "        \n",
    "        # Upsample\n",
    "        self.up2 = ResidualBlock1D(base_dim*3, base_dim, base_dim + cond_dim) # Concat skip\n",
    "        self.up1 = ResidualBlock1D(base_dim*2, base_dim, base_dim + cond_dim)\n",
    "        \n",
    "        # Output Heads\n",
    "        # Head 1: Continuous noise prediction (Features 0-5)\n",
    "        self.head_cont = nn.Conv1d(base_dim, 5, 1)\n",
    "        # Head 2: Discrete Role logits (Features 6-9 -> 4 classes)\n",
    "        self.head_disc = nn.Conv1d(base_dim, 5, 1)\n",
    "\n",
    "    def forward(self, x, t, conditions):\n",
    "        # 1. Process Time and Conditions\n",
    "        # t is [B, 1], conditions is [B, 4]\n",
    "        t_emb = self.time_mlp(t)\n",
    "        # Concatenate time embedding with global conditions\n",
    "        global_cond = torch.cat([t_emb, conditions], dim=1) \n",
    "        \n",
    "        # 2. U-Net Backbone\n",
    "        x = x.transpose(1, 2) # [B, L, C] -> [B, C, L]\n",
    "        x1 = self.init_conv(x)\n",
    "        \n",
    "        x2 = self.down1(x1, global_cond)\n",
    "        x3 = self.down2(F.max_pool1d(x2, 2), global_cond)\n",
    "        \n",
    "        x_mid = self.mid(x3, global_cond)\n",
    "        \n",
    "        x_up2 = F.interpolate(x_mid, scale_factor=2)\n",
    "        x_up2 = torch.cat([x_up2, x2], dim=1) # Skip connection\n",
    "        x_up2 = self.up2(x_up2, global_cond)\n",
    "        \n",
    "        x_up1 = torch.cat([x_up2, x1], dim=1)\n",
    "        x_out = self.up1(x_up1, global_cond)\n",
    "        \n",
    "        # 3. Split Outputs\n",
    "        pred_noise = self.head_cont(x_out)   # [B, 5, 20]\n",
    "        pred_logits = self.head_disc(x_out)  # [B, 5, 20]\n",
    "        \n",
    "        return pred_noise.transpose(1, 2), pred_logits.transpose(1, 2)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. The Hybrid Diffusion Wrapper\n",
    "# -------------------------------------------------------------------\n",
    "class HybridDDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        timesteps=1000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = ClimbingUNet1D()\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Feature indices\n",
    "        self.cont_idx = slice(0, 5) # x, y, pull_x, pull_y, is_foot\n",
    "        self.disc_idx = slice(5, 10) # 4 one-hot roles\n",
    "    \n",
    "    def _sine_alpha_bar(self, t):\n",
    "        \"\"\"Compute alpha bar using sine schedule.\"\"\"\n",
    "        a = torch.sin(t*torch.pi/2)**2\n",
    "        return a.reshape((-1,1,1))\n",
    "    \n",
    "    def q_sample(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Hybrid Noise Injection:\n",
    "        - Continuous: Add Gaussian Noise\n",
    "        - Discrete: Apply Absorbing State (Masking)\n",
    "        \"\"\"\n",
    "        B, L, C = x_start.shape\n",
    "        a = self._sine_alpha_bar(t)\n",
    "        \n",
    "        # --- Part A: Continuous Features ---\n",
    "        x_cont = x_start[:, :, self.cont_idx]\n",
    "        noise = torch.randn_like(x_cont)\n",
    "        \n",
    "        # Gaussian Diffusion Formula: sqrt(a_bar)*x0 + sqrt(1-a_bar)*eps\n",
    "        noisy_cont = torch.sqrt(a) * x_cont + torch.sqrt(1 - a) * noise\n",
    "        \n",
    "        # --- Part B: Discrete Features (Absorbing State) ---\n",
    "        x_disc = x_start[:, :, self.disc_idx] # One-hot [B, L, 5]\n",
    "        \n",
    "        mask_prob = torch.rand(B,L) > a.reshape((-1,1))\n",
    "        x_disc[mask_prob, :] = 0.0\n",
    "        \n",
    "        # Combine\n",
    "        x_t = torch.cat([noisy_cont, x_disc], dim=2)\n",
    "        \n",
    "        return x_t, noise\n",
    "\n",
    "    def loss(self, x_start, conditions):\n",
    "        batch_size = x_start.shape[0]\n",
    "        device = x_start.device\n",
    "        t = torch.rand((batch_size, 1), device=device)\n",
    "\n",
    "        x_t, noise_target = self.q_sample(x_start, t)\n",
    "        \n",
    "        pred_noise, pred_logits = self.model(x_t, t, conditions)\n",
    "        loss_cont = F.mse_loss(pred_noise, noise_target)\n",
    "\n",
    "        target_classes = torch.argmax(x_start[:, :, self.disc_idx], dim=2).reshape(-1)\n",
    "        pred_logits = pred_logits.reshape((-1,5))\n",
    "        loss_disc = F.cross_entropy(pred_logits, target_classes)\n",
    "        \n",
    "        return loss_cont + loss_disc\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, conditions, device):\n",
    "        B = conditions.shape[0]\n",
    "        L = 20\n",
    "        # Initialize noisy data\n",
    "        x_cont = torch.randn(B, L, 5, device=device)\n",
    "        x_disc = torch.zeros(B, L, 5, device=device)\n",
    "        \n",
    "        # 2. Denoising Loop\n",
    "        for i in range(self.timesteps):\n",
    "            t = i/self.timesteps\n",
    "            t_tensor = torch.full((B, 1), t, device=device, dtype=torch.float32)\n",
    "            x_in = torch.cat([x_cont, x_disc], dim=2)\n",
    "\n",
    "            # Predict\n",
    "            x_cont, x_disc = self.model(x_in, t_tensor, conditions)\n",
    "            if i==self.timesteps-1:\n",
    "                break\n",
    "\n",
    "            # Add noise back into the continuous features according to schedule\n",
    "            a = self._sine_alpha_bar(t)\n",
    "            x_cont = torch.sqrt(a) * x_cont + torch.sqrt(1-a) * torch.randn(B, L, 5, device=device)\n",
    "\n",
    "            # Add noise back into logit predictions by randomly re-masking them according to schedule.\n",
    "            mask_decision = torch.rand(B,L) > a.reshape((-1,1))\n",
    "            x_disc[mask_decision,:] = 0.0\n",
    "\n",
    "        return torch.cat([x_cont, x_disc], dim=2)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. The Trainer\n",
    "# -------------------------------------------------------------------\n",
    "class HybridDDPMTrainer():\n",
    "    \"\"\"Trainer for the Hybrid DDPM Climb Diffusion Model\"\"\"\n",
    "    def __init__(self, model, dataset, device='cpu'):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        self.device = torch.device(device)\n",
    "        self.dataset = TensorDataset(*[torch.Tensor(arr) for arr in dataset])\n",
    "    \n",
    "    def train_model(self, epochs=500, batch_size=64):\n",
    "        \"\"\"Train the model for n epochs with batching\"\"\"\n",
    "        batches = DataLoader(self.dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
    "        self.model.train()\n",
    "        with tqdm(range(epochs)) as pbar:\n",
    "            for epoch in pbar:\n",
    "                total_loss = 0\n",
    "                for x, c in batches:\n",
    "                    x = x.to(self.device)\n",
    "                    c = c.to(self.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    # Calculate loss (Forward Diffusion + Prediction)\n",
    "                    loss = self.model.loss(x,c)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                pbar.set_postfix_str(f\"Epoch: {epoch}, Batches:{len(batches)} Total Loss: {total_loss:.4f}, Avg Loss: {total_loss/len(batches):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2bda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "climbs = ClimbsFeatureArray(db_path='../data/storage.db')\n",
    "dataset = climbs.get_features_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridDDPM()\n",
    "trainer = HybridDDPMTrainer(\n",
    "    model = model,\n",
    "    dataset = dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train_model(epochs=1500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
